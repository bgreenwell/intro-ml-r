---
output:
  xaringan::moon_reader:
    seal: false
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: [default, custom.css, hygge, "ninjutsu"]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: true
---
class: clear

.font300[

Introduction to Machine learning in `r icon::fa("r-project")` `r anicon::cia("https://emojis.slackmojis.com/emojis/images/1450319453/122/carlton.gif?1450319453", animate = FALSE, size = 1, border = FALSE)`

]

.font250.right[

Analytics Connect '18

]

.pull-left.font120[

Brandon M. Greenwell<br>Enable the Science/Data Scientist<br>[`r anicon::faa("github", animate = "vertical", rtext = " Fork me on GitHub")`](https://github.com/bgreenwell/intro-ml-r)<br>[`r anicon::faa("twitter", animate = "float", rtext = " @bgreenwell8")`](https://twitter.com/bgreenwell8)

]

.pull-right[

```{r effo, echo=FALSE, out.width="100%"}
knitr::include_graphics("figures/effo-logo.png")
```


]

---
class: clear, split-two

```{r setup, include=FALSE}
# Set global R options
options(htmltools.dir.version = FALSE, servr.daemon = TRUE, 
        crayon.enabled = TRUE)

# Set global knitr chunk options
knitr::opts_chunk$set(
  dev = "svg",
  fig.align = "center",
  cache = TRUE,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE 
)

# Load required packages
library(ggplot2)
library(gganimate)
library(pdp)
library(vip)

# Colors
dark2 <- RColorBrewer::brewer.pal(8, name = "Dark2")

# Brandon's RBitmoji id
my_id <- "1551b314-5e8a-4477-aca2-088c05963111-v1"
```

# Overview

.column.bg-main1[.content.vmiddle.center[

<center>
 <img src="figures/isl.jpg", width="70%">

.font150[[Book website](http://www-bcf.usc.edu/~gareth/ISL/)]

]]

.column.bg-main2[.content.vmiddle.center[

<center>
 <img src="figures/esl.jpg", width="70%">

.font150[[Book website](https://web.stanford.edu/~hastie/ElemStatLearn/)]

]]


---
class: clear, center, middle

.pull-left[

.font300.bold[What is machine learning?]

]

.pull-right[

![](figures/ml-tshirt.jpg)

]


---

# Popular terms

.font150[

* .purple[Machine learning]: constructing algorithms that learn from data

* .purple[Statistical learning]: emphasizes statistical models and the assessment of uncertainty

* .purple[Data science]: applying mathematics, .bold[statistics], machine learning, engineering, etc. to extract knowledge form data

    - "Data Science is statistics on a Mac" `r icon::fa("apple")`

]

.full-width.content-box-red.font150.center[Similar terms with slightly different emphases!]


---

# Some take home messages

.full-width.content-box-purple.font150.center[This talk is about *supervised learning*: building models from data that predict an **outcome** using a collection of **features**]

.font150[

* Many flexible `r emo::ji("hammer and wrench")` for making predictions from data

* They are not magic! They require `r anicon::nia("**good data**", animate = "pulse")` and proper validation!

* Model fitting (and validation) takes longer with "big data" `r emo::ji("time")`

* No one method is uniformly best!

]


---
class: clear, middle, center

.font300[

Follow along: [`r icon::fa("r-project")` code](https://github.com/bgreenwell/intro-ml-r/blob/master/code/intro-ml-r.R)

]


---
class: clear, inverse, center, middle

```{r fundamentals, echo=FALSE, out.width="100%"}
knitr::include_graphics("figures/fundamentals.jpg")
```


---

# Classification vs. regression

```{r classification-vs-regression, echo=FALSE, fig.width=7, fig.asp=0.5, out.width="100%"}
# Set up plotting grid
par(mfrow = c(1, 2))

# Boston housing data
plot(
  formula = cmedv ~ lstat, 
  data = pdp::boston, 
  # asp = 1,
  cex = 1.2,
  pch = 19,
  col = adjustcolor(dark2[1], alpha.f = 0.1),
  main = "Regression",
  xlab = "Lower status of the population (%)",
  ylab = "Median home value"
)
mars <- earth::earth(cmedv ~ lstat, data = pdp::boston)
x <- seq(from = 1.73, max = 37.97, length = 100)
y <- predict(mars, newdata = data.frame(lstat = x))
lines(x, y)

# Simulate data
set.seed(805)
norm2d <- as.data.frame(mlbench::mlbench.2dnormals(
  n = 100,
  cl = 2,
  r = 4,
  sd = 1
))
names(norm2d) <- c("x1", "x2", "y")  # rename columns
# Scatterplot
plot(
  formula = x2 ~ x1, 
  data = norm2d, 
  asp = 1,
  cex = 1.2,
  pch = c(17, 19)[norm2d$y],
  col = adjustcolor(dark2[norm2d$y], alpha.f = 0.5),
  main = "Classification",
  xlab = "Income (standardized)",
  ylab = "Lot size (standardized)"
)
abline(a = 0, b = -1)
legend("topleft", pch = c(17, 19), col = dark2[1:2], cex = 0.5, bty = "n",
       inset = 0.01, title = "Owns a riding mower", legend = c("Yes", "No"))
```



---

# Example data sets

<!-- .font250.center[[Data set information](https://bgreenwell.github.io/intro-ml-r/datasets)] -->

.font175[

.pull-left[

## Classification:

* Mushroom edibility `r emo::ji("mushroom")`

* E-mail spam `r emo::ji("email")`

* Swiss banknote `r set.seed(8); emo::ji("currency")`

]

.pull-right[

## Regression:

* Boston housing data `r emo::ji("house")`

* Ames housing data `r emo::ji("house")`

]

]


---

# Nature as a black box

.font200[

Nature acts on $\boldsymbol{X}$ to produce $\boldsymbol{Y}$:

]

```{r black-box-nature, echo=FALSE, out.width="90%"}
knitr::include_graphics("figures/black-box-nature.svg")
```


---

# Data modeling culture

.font200[

Assume a stochastic data model:

]

```{r data-modeling-culture, echo=FALSE, out.width="90%"}
knitr::include_graphics("figures/data-modeling-culture.svg")
```

???

Domain experts work for years to learn good features; they bring the statistician a "small" data set


---

# Algorithmic modeling culture

.font200[

Find a function $f\left(\boldsymbol{X}\right)$ to predict $\boldsymbol{Y}$:

]

```{r algorithmic-modeling-culture, echo=FALSE, out.width="90%"}
knitr::include_graphics("figures/algorithmic-modeling-culture.svg")
```

???

We start with a "large" data set with many features and use machine learning to find the "good" ones


---

# Algorithmic interpretation `r anicon::cia("figures/vip-pdp-horizontal.png", animate = FALSE, size = 1)`

.font150[

Aside from predictions, we are often interested in understanding the way that $Y$ is affected as $X_1, \dotsc, X_p$ change (i.e., we cannot treat $\widehat{f}$ as a "black
`r emo::ji("package")`"). 

]

.font125[

.pull-left[

<!-- In this setting, one may be interested in answering the following questions: -->

* Which predictors are .bold.blue[associated] with the response?

* What is the nature of the .bold.blue[relationship] between the response and each predictor (or subset thereof)?

]

.pull-right[

```{r vip-pdp-diag, echo=FALSE, out.width="100%"}
knitr::include_graphics("figures/vip-pdp-horizontal.png")
```

]

]


---

# Algorithmic interpretation `r anicon::cia("figures/vip-pdp-horizontal.png", animate = FALSE, size = 1)`

<br>

```{r black-box-02, echo=FALSE, out.width="100%"}
knitr::include_graphics("figures/black-box-02.png")
```


---

# Algorithmic interpretation `r anicon::cia("figures/vip-pdp-horizontal.png", animate = "pulse", size = 1)`

<br>

```{r black-box-03, echo=FALSE, out.width="100%"}
knitr::include_graphics("figures/black-box-03.png")
```


---

# Over fitting

.font150[

It is possible to "over tune" your models to maximize performance on the training data. Such models will not generalize well to new data. We call this process [*over fitting*](https://en.wikipedia.org/wiki/Overfitting):

]

```{r overfitting-linear-regression, echo=FALSE, fig.width=7, fig.height=7/3, out.width="100%"}
# Simulate some data 
n <- 100
set.seed(8451)
df <- tibble::tibble(
  x = runif(n, min = -2, max = 2),
  y = rnorm(n, mean = 1 + 2*x + x^2, sd = 1)
)
p <- ggplot(df, aes(x, y)) + 
  geom_point(alpha = 0.5) + 
  theme_light()
p1 <- p + 
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
  ggtitle("Under fitting")
p2 <- p + 
  geom_smooth(method = "lm", formula = y ~ x + I(x^2), se = FALSE) +
  ggtitle("Just right?")
p3 <- p + 
  geom_smooth(method = "loess", span = 0.1, se = FALSE) +
  ggtitle("Over fitting")
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```


---

# Model development

.font150.pull-left[

## Model selection

Estimating the *performance* of different models in order
to choose the .bold["best"] one

* Choosing *hyperparmaters*

]

.font150.pull-right[

## Model assessment

Having chosen a final model, estimating its prediction
error (i.e., generalization error) on .bold[new data]

]

.full-width.content-box-yellow.font175.center[If we have "enough" data, divide it up!]


---

# Data splitting

.full-width.content-box-yellow.font150.center[How do we divide up the data for each task?]

.font135[

* The .bold[training set] is used to fit the models

* The .bold[validation set] is used to estimate prediction error for model selection

* The .bold[test set] is used for assessment of the generalization error of the final chosen model

    - Ideally, the test set should be kept under `r emo::ji("lock")` and `r emo::ji("key")` until the end of the data analysis (**Why?** `r emo::ji("thinking")`)

]


---

# Data splitting

.font150[

It is difficult to give a general rule of `r emo::ji("thumbs up")` on how to choose the number of observations in each of the three parts, as this depends on the *signal-to noise ratio* in the data and the training sample size

]

.full-width.content-box-yellow.font150.center[A typical split might be 50% for training, and 25% each for validation and testing]


---

# Data splitting strategies

<br>

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("figures/data-splitting.png")
```


---

# 5-fold cross-validation

<br><br>

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("figures/cross-validation.png")
```


---
class: clear, middle, center

.font200[

Generalization error $\approx$ $\sigma^2$ $+$ .red.vold[Bias] $+$ .red.vold[Variance] 

]

```{r bias-variance-tradeoff, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="80%"}
# Load required packages
library(mlbench)
library(ggplot2)
library(tidyr)

# Simulate data
set.seed(8451)
trn <- as.data.frame(mlbench.friedman1(1000, sd = 1))#[, c(1:5, 11)]
tst <- as.data.frame(mlbench.friedman1(1000, sd = 1))#[, c(1:5, 11)]

# Run K-nn simulation
k <- seq(from = 2, to = 30, by = 1)
res <- matrix(NA, nrow = length(k), ncol = 3, 
              dimnames = list(NULL, c("k", "Train", "Test")))
for (i in seq_along(k)) {
  fit <- caret::knnreg(y ~ ., k = k[i], data = trn)
  res[i, "k"] <- k[i]
  res[i, "Train"] <- caret::RMSE(predict(fit, newdata = trn), trn$y)
  res[i, "Test"] <- caret::RMSE(predict(fit, newdata = tst), tst$y)
}
res <- res %>%
  as.data.frame() %>%
  gather(key = "sample", value = "rmse", -k)

# Plot results
best <- min(res[res[, "sample"] == "Test", "rmse"])
ggplot(res, aes(x = 1/k, y = 1/rmse, color = sample)) +
  geom_line(size = 1) +
  geom_hline(yintercept = 1/best, linetype = "dashed") +
  scale_x_log10() +
  scale_y_log10() +
  xlab("Model complexity") +
  ylab("Model performance") +
  theme_light() +
  ggtitle("Bias-variance tradeoff") +
  annotate("text", x = 0.042, y = 0.44, label = "High Bias\nLow Variance", 
           col = "grey30", size = 5) +
  annotate("text", x = 0.39, y = 0.44, label = "Low Bias\nHigh Variance",
           col = "grey30", size = 5) +
  theme(legend.title = element_blank(),
        legend.position = c(0.1, 0.85),
        legend.text = element_text(size = 10),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```


---

# Data splitting in R

.code125[

```{r sampling}
library(rsample)  # for data splitting

# Make sure that you get the same random numbers
set.seed(123)
data_split <- initial_split(
  pdp::boston, prop = 0.7, strata = "cmedv"
)

boston_train <- training(data_split)  # train
boston_test  <- testing(data_split)  #test

nrow(boston_train)/nrow(pdp::boston)
```

]


---

# Outcome distribution

```{r sampling-distribution, fig.width=6, fig.asp=0.618, out.width="60%"}
library(ggplot2)  # for awesome graphics

# Do the response distributions seem to differ?
ggplot(boston_train, aes(x = cmedv)) +
  geom_density(trim = TRUE) +
  geom_density(data = boston_test, trim = TRUE, col = "red")
```


---

# Data shape

```{r wide-data-tall-data, echo=FALSE, out.width="40%"}
knitr::include_graphics("figures/wide-data-tall-data.png")
```


---

# Wide data (lots of features)

.pull-left.center[

```{r wide-data, echo=FALSE, out.width="100%"}
knitr::include_graphics("figures/wide-data.png")
```

]

.pull-right.font125[

* Thousands / millions of features

* Hundreds of records (i.e., rows)

* Prone to over fitting 

    - Need to regularize or otherwise reduce the number features

]

.full-width.content-box-yellow.font150.center[Screening with FDR, Elastic net, SVMs, and stepwise procedures]

---

# Tall data (lots of observations)

.pull-left.center[

```{r tall-data, echo=FALSE, out.width="30%"}
knitr::include_graphics("figures/tall-data.png")
```

]

.pull-right.font125[

* Tens / hundreds of features

* Thousands / millions of records (i.e., rows)

* Plenty of samples to fit richer models with interactions, etc.!

]

.full-width.content-box-yellow.font150.center[GzLMs, .bold.red[tree-based methods] (like GBMs), and network models (like DNNs)]


---

# Data preprocessing and feature engineering

.pull-left[

.font100[

Once data are gathered, it must be cleaned and prepped for data analysis. Such tasks might include, but are not limited to:

* Imputing missing values

* Transforming columns

* Standardizing/normalizing columns

* Creating new features

* Formulating interaction effects

* Numerically encoding categorical features (e.g., *one-hot encoding* or *dummy encoding*)

]

]

.pull-right[

```{r ames-sale-price, echo=FALSE, fig.width=4, fig.asp=2*0.618, out.width="100%"}
p1 <- ggplot(ames, aes(x = Sale_Price, y = ..density..)) +
  geom_histogram(bins = 20, fill = "dodgerblue2", color = "white") +
  # geom_density() +
  scale_x_continuous(name = "Sale price ($K)",
                     labels = function(x) paste0(floor(x/1000))) +
  theme_light() +
  ggtitle("Skew right")
p2 <- ggplot(ames, aes(x = log(Sale_Price), y = ..density..)) +
  geom_histogram(bins = 20, fill = "dodgerblue2", color = "white") +
  # geom_density() +
  theme_light() +
  ggtitle("More symmetric")
grid.arrange(p1, p2, ncol = 1)
```

]


---
class: clear, middle, center

```{r sklearn-chart, echo=FALSE, out.width="100%"}
knitr::include_graphics("figures/flow-chart.png")
```

.font150[

**Source:** [Scikit-learn](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)

]


---
class: clear, middle, center

```{r apm-chart, echo=FALSE, out.width="100%"}
knitr::include_graphics("figures/apm-summary.png")
```

.font150[

**Source:** [Applied Predictive Modeling](http://appliedpredictivemodeling.com/)

]


---

# A good modeling tool

.pull-left.font110[

## <span style="color:MediumSeaGreen">At a minimum:</span>

* Applicable to classification and regression
    
* Competitive accuracy
    
* Capable of handling large data sets
    
* Handle missing values effectively

]


--

.pull-right.font110[

## <span style="color:MediumSeaGreen">Bonus features:</span>

* Which predictors are important? ([vip](https://github.com/AFIT-R/vip))

* How do the predictors functionally relate to the response? ([pdp](https://journal.r-project.org/archive/2017/RJ-2017-016/index.html))
    
* How do the predictors interact? ([pdp](https://journal.r-project.org/archive/2017/RJ-2017-016/index.html), [vip](https://github.com/AFIT-R/vip))
    
* What is the shape of the data (i.e., how does it cluster?)
    
* Are their novel cases and outliers?

]
    

---
class: clear, middle, center

background-image: url(https://upload.wikimedia.org/wikipedia/commons/9/99/Fog_forrest_frickberg.jpg)

.font300.white[Decision trees]

???

Image credit: [Wikimedia Commons](https://commons.wikimedia.org/wiki/Forest#/media/File:Fog_forrest_frickberg.jpg)

---

# Mushroom classification

There is no simple rule for determining the edibility of a [mushroom](https://raw.githubusercontent.com/bgreenwell/MLDay18/master/docs/data/mushroom.csv); no rules like **"leaflets three, let it be"**, **"hairy vine, no friend of mine"** and **"berries white, run in fright"** for poison ivy.

```{r, echo=FALSE, out.width="50%"}
knitr::include_graphics("figures/edible.jpg")
```

--

```{r mushroom-load, echo=TRUE}
url <- "https://bgreenwell.github.io/MLDay18/data/mushroom.csv"
mushroom <- read.csv(url)  # load the data from GitHub
mushroom$veil.type <- NULL  # only takes on a single value  #<<
```


---
class: clear, middle

# Mushroom classification

.code125[

```{r mushroom-setup, echo=TRUE}
# Load required packages
library(caret)  # for data splitting function
library(rpart)  # for binary recursive partitioning

# Partition the data into train/test sets
set.seed(101)
trn_id <- createDataPartition(
  y = mushroom$Edibility, p = 0.5, list = FALSE
)
trn <- mushroom[trn_id, ]   # training data
tst <- mushroom[-trn_id, ]  # test data

# Function to calculate accuracy
accuracy <- function(pred, obs) {
  sum(diag(table(pred, obs))) / length(obs)
}
```

]


---
class: clear, middle

# Mushroom classification

.pull-left[

```{r mushroom-tree-1}
# Decision stump (test error = 1.53%):
cart1 <- rpart(
  Edibility ~ ., data = trn,
  control = rpart.control(maxdepth = 1) #<<
)

# Get test set predictions
pred1 <- predict(
  cart1, newdata = tst, 
  type = "class"
)

# Compute test set accuracy
accuracy(
  pred = pred1, 
  obs = tst$Edibility
)
```

]

.pull-right[

```{r mushroom-tree-2}
# Optimal tree (test error = 0%):
cart2 <- rpart(
  Edibility ~ ., data = trn, 
  control = list(cp = 0, minbucket = 1, minsplit = 1) #<<
)

# Get test set predictions
pred2 <- predict(
  cart2, newdata = tst, 
  type = "class"
)

# Compute test set accuracy
accuracy(
  pred = pred2, 
  obs = tst$Edibility
)
```

]

```{r mushroom-cm, include=FALSE}
# Test set confusion matrices
confusionMatrix(pred1, tst$Edibility)
confusionMatrix(pred2, tst$Edibility)
```


---

# Mushroom classification

.pull-left[

.font125[Decision stump (test error = 1.53%):]
```{r mushroom-tree-diagram-1, echo=FALSE, out.width="100%"}
# Load required packages
library(rpart.plot)

# Tree diagram (deep tree)
prp(cart1,
    type = 4,
    clip.right.labs = FALSE, 
    branch.lwd = 2,
    extra = 1, 
    under = TRUE,
    under.cex = 1.5,
    split.cex = 1.5,
    box.col = c("palegreen3", "pink")[cart1$frame$yval])
```

]

.pull-right[

.font125[Optimal tree (test error = 0%):]
```{r mushroom-tree-diagram-2, echo=FALSE, out.width="100%"}
# Tree diagram (shallow tree)
prp(cart2,
    type = 4,
    clip.right.labs = FALSE, 
    branch.lwd = 2,
    extra = 1, 
    under = TRUE,
    under.cex = 1.5,
    split.cex = 1.5,
    box.col = c("palegreen3", "pink")[cart2$frame$yval])
```

]


---

# A handy rule of `r emo::ji("thumbs up")` for mushrooms

<br><br><br><br>

.right.font300.tomato["If it smells bad, don't eat it!"]
<br>
.right.font200[--- Decision Stump]


---

# Predicting email spam

.pull-left[

- Data from 4601 email messages collected at Hewlett-Packard Labs

- **Goal:** predict whether an email message is <span style="color:Tomato">spam</span> (junk email) or <span style="color:MediumSeaGreen ">ham</span> (good email)

- **Features:** relative frequencies in a message of 57 of the most commonly occurring words and punctuation marks in all the training the email messages

- For this problem, not all errors are equal; misclassifying <span style="color:Tomato">spam</span> is not as bad as misclassifying <span style="color:MediumSeaGreen">ham</span>!

]

.pull-right[

```{r fig-spam-300, echo=FALSE, out.width="100%"}
knitr::include_graphics("figures/this-is-spam.jpg")
```

]


---

# Predicting email spam

```{r spam-tree}
# Load the data
data(spam, package = "kernlab") #<<

# Partition the data into train/test sets
set.seed(101)  # for reproducibility
trn_id <- createDataPartition(spam$type, p = 0.7, list = FALSE)
trn <- spam[trn_id, ]                # training data
tst <- spam[-trn_id, ]               # test data
xtrn <- subset(trn, select = -type)  # training data features
xtst <- subset(tst, select = -type)  # test data features
ytrn <- trn$type                     # training data response

# Fit a classification tree (cp found using k-fold CV)
spam_tree <- rpart(type ~ ., data = trn, cp = 0.001) #<<
pred <- predict(spam_tree, newdata = xtst, type = "class")

# Compute test set accuracy
(spam_tree_acc <- accuracy(pred = pred, obs = tst$type))
```


---

# Predicting email spam

.scrollable[

```{r spam-tree-diagram, echo=FALSE, out.width="70%"}
# Tree diagram
prp(spam_tree,
    type = 4,
    clip.right.labs = FALSE, 
    branch.lwd = 2,
    extra = 1, 
    under = TRUE,
    under.cex = 1.5,
    split.cex = 1.5,
    box.col = c("palegreen3", "pink")[spam_tree$frame$yval])
```

]


---

# Predicting email spam

.pull-left[

```{r spam-vi}
# Variable importance scores
vip::vi(spam_tree) %>%  #<<
  as.data.frame() %>%
  head(10)
``` 

]

.pull-right[

```{r spam-vip, fig.width=6, fig.asp=0.618, out.width="100%"}
# Variable importance plot
vip(spam_tree, num_features = 10)  #<<
```

]


---

# How do trees measure up?

.pull-left[

.font150.bold[Advantages:]

* .MediumSeaGreen[Small trees are easy to interpret]

* .MediumSeaGreen[Trees scale well to large *N*] (fast!!)

* .MediumSeaGreen[Can handle data of all types] (i.e., requires little, if any, coding)

* .MediumSeaGreen[Automatic variable selection]

* .MediumSeaGreen[Can handle missing data] (through *surrogate splits*)

* .MediumSeaGreen[Completely nonparametric] (great for DM and EDA tasks!)

]

--

.pull-right[

.font150.bold[Disadvantages:]

* .tomato[Large trees can be difficult to interpret]

* .tomato[Trees are step functions] (i.e., binary splits)

* .tomato[Greedy splitting algorithms] (i.e., trees are noisy)

* .tomato[All splits depend on previous splits] (i.e., high order interactions)

* .tomato[Data is essentially taken away after each split]

]


---
class: clear, middle, center

# How do trees measure up?

```{r fig-characteristics, echo=FALSE, out.width="70%"}
knitr::include_graphics("figures/esl-tbl.png")
```
<br>
.center[**Source:** The Elements of Statistical Learning, Second Edition]


---

# Creating ensembles of trees

* The key to accuracy is **low bias** and **low variance**

* **Main idea:** breaking the [*bias-variance trade-off*](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)

    - $Error \approx Bias + Variance$

* Two popular algorithms (can be applied to any model, **not just trees!**):

    - Bagging (<span style="color:Magenta">reduce variance by averaging</span>)

    - Boosting (<span style="color:Magenta">reduce bias by building models sequentially</span>)
    
* Random forest is just a slight modification over bagging applied to decision trees!

<br>

<center>
<font size="6"><span style="color:DarkBlue">Boosting</span> >= <span style="color:MediumSeaGreen">Random forest</span> > <span style="color:MediumOrchid ">Bagging</span> > <span style="color:DarkOrange">Single tree</span></font>
</center>


---
class: clear, center, middle

background-image: url(https://upload.wikimedia.org/wikipedia/commons/9/99/Fog_forrest_frickberg.jpg)

.font300.white[Bagging]

???

Image credit: [Wikimedia Commons](https://commons.wikimedia.org/wiki/Forest#/media/File:Fog_forrest_frickberg.jpg)


---

# Background

.pull-left[

* **B**ootstrap **agg**regat**ing**

* Bagging trees:

    1. Sample records **with replacement** (i.e., bootstrap the training data)
    
    2. Fit an **overgrown tree** to the resampled data set
    
    3. Repeat a large number of times ( $\ge 500$ , say)

]
 

.pull-right[

```{r fig-bagged-trees, echo=FALSE, out.width="80%"}
knitr::include_graphics("figures/millionaire.png")
```

]
* Predictions are combined by popular vote (**classification**) or averaging (**regression**)

* Same idea as ["wisdom of the crowd"](https://en.wikipedia.org/wiki/Wisdom_of_the_crowd) (<span style="color:purple">Francis Galton's Ox weight survey</span>)

* Improves the stability and accuracy of noisy models (e.g., individual trees)


---

# Bagging trees

```{r bagging-1, echo=FALSE, out.width="60%"}
# Simulate some sine wave data
set.seed(1112)  # for reproducibility
x <- seq(from = 0, to = 2 * pi, length = 100)
y <- sin(x) + rnorm(length(x), sd = 0.5)
plot(x, y)
lines(x, sin(x))
legend("topright", legend = "True function", lty = 1, inset = 0.01,
       box.col = "transparent")
```


---

# Trees are step functions

```{r bagging-2, echo=FALSE, out.width="60%"}
# Fit a single regression tree
fit <- rpart(y ~ x, cp = 0)
pred <- predict(fit)
plot(x, y)
lines(x, sin(x))
cols <- RColorBrewer::brewer.pal(9, "Set1")
lines(x, pred, col = cols[1L], lwd = 2)
lgnd <- c("True function", "Single tree")
legend("topright", legend = lgnd, col = c("black", cols[1L]), 
       lty = 1, inset = 0.01, box.col = "transparent")
```


---

# Trees are noisy

```{r bagging-3, echo=FALSE, out.width="60%"}
# Fit many regression trees to bootstrap samples
plot(x, y)
nsim <- 1000
pred_mat <- matrix(nrow = length(x), ncol = nsim)
set.seed(1145)  # for reproducibility
id <- replicate(nsim, sort(sample(length(x), replace = TRUE)))
for (i in 1:nsim) {
  fit <- rpart(y[id[, i]] ~ x[id[, i]], cp = 0)
  pred_mat[, i] <- predict(fit)
  lines(x[id[, i]], pred_mat[, i], 
        col = adjustcolor(cols[2L], alpha.f = 0.05))
}
lines(x, sin(x))
lines(x, pred, col = cols[1L], lwd = 2)
lgnd <- c("True function", "Single tree", "Bootstrapped tree")
legend("topright", legend = lgnd, col = c("black", cols[1L:2L]), 
       lty = 1, inset = 0.01, box.col = "transparent")
```


---

# Bagging reduces variance

```{r bagging-4, echo=FALSE, out.width="60%"}
plot(x, y)
for (i in 1:nsim) {
  lines(x[id[, i]], pred_mat[, i], 
        col = adjustcolor(cols[2L], alpha.f = 0.05))
}
lines(x, sin(x))
lines(x, pred, col = cols[1L], lwd = 2)
lines(x, apply(pred_mat, MARGIN = 1, FUN = mean), col = cols[6L], lwd = 2)
lgnd <- c("True function", "Single tree", "Bootstrapped tree", "Averaged trees")
legend("topright", legend = lgnd, col = c("black", cols[c(1, 2, 6)]), lty = 1, 
       inset = 0.01, box.col = "transparent")
```


---

# Predicting email spam

.pull-left[

```{r}
# Load required packages
library(randomForest)  #<<

# Fit a bagger model
xtst <- subset(tst, select = -type)  #<<
set.seed(1633)  # reproducibility  #<<
spam_bag <- randomForest(
  type ~ ., 
  data = trn, 
  ntree = 250,
  mtry = ncol(xtrn),  #<<
  xtest = xtst,
  ytest = tst$type,
  keep.forest = TRUE
)
```

]

.pull-right[

```{r, echo=FALSE, fig.width=4, fig.asp=1, out.width="100%"}
# Compute test error
pred <- predict(spam_bag, newdata = xtst, type = "class")
spam_bag_acc <- accuracy(pred = pred, obs = tst$type)

# Tidy test error data
df <- spam_bag$test$err.rate %>%
  as.data.frame() %>%
  tibble::rowid_to_column("num_trees") %>%
  tidyr::gather(type, error, -num_trees) %>%
  dplyr::mutate(
    type = dplyr::case_when(
      type == "nonspam" ~ "Non-spam",
      type == "spam" ~ "Spam",
      TRUE ~ "Overall",
    )
  )

# Animate test error progression
p <- ggplot(df, aes(x = num_trees, y = error, color = type)) +
  geom_path() +
  scale_colour_brewer(palette = "Dark2") +
  geom_hline(yintercept = 1 - spam_tree_acc, linetype = 2, color = "black") +
  geom_hline(yintercept = 1 - spam_bag_acc, linetype = 2, color = dark2[2L]) +
  theme_light() +
  theme(legend.title = element_blank(),
        legend.position = c(0.9, 0.9)) +
  xlab("Number of trees") +
  ylab("Test error") +
  ylim(0.04, 0.11) +
  transition_reveal(id = type, along = num_trees)
animate(p, renderer = gifski_renderer(), device = "png")
```

]


---
class: clear, center, middle

background-image: url(https://upload.wikimedia.org/wikipedia/commons/9/99/Fog_forrest_frickberg.jpg)

.font300.white[Random forest]

???

Image credit: [Wikimedia Commons](https://commons.wikimedia.org/wiki/Forest#/media/File:Fog_forrest_frickberg.jpg)


---

# Background

* **Bagging on steroids!!**

* Correlation limits the effect of bagging!

* Grow trees just as in bagging, but with a small twist

    - Before each split in each tree in the forest, select a subset of the predictors **at random** as candidate splitters
    
      - This essentially **"decorrelates"** the trees in the ensemble!
    
    - The number of randomly selected variables, denoted $m_{try}$, is a **tuning parameter** (arguably the only tuning parameter in a random forest)

* Bagging introduces <span style="color:CornFlowerBlue">randomness into the rows</span> of the data

* Random forest introduces <span style="color:CornFlowerBlue">randomness into the rows and columns</span> of the data

* [*Extremely randomized trees*](https://link.springer.com/article/10.1007/s10994-006-6226-1) take this randomness one step further to reduce variance!


---

# Random forest packages in R

.scrollable[

.pull-left[

* [randomForest](https://cran.r-project.org/package=randomForest)

    - The standard (implements most of the **bells and whistles**; great for EDA!!)
    
    - Classification and regression
    
    - Does not scale well, but can be parallelized using `foreach`!

* [randomForestSRC](https://kogalur.github.io/randomForestSRC/index.html)

    - Classification, regression, and survival analysis (including ***competing risks***)
    
    - MPI/OpenMP support

* [ranger](https://github.com/imbs-hl/ranger): **ran**dom forest **ge**ne**r**ator

    - Classification, regression, and survival analysis
    
    - [Fast!](https://www.jstatsoft.org/article/view/v077i01)
    
    - Accepts sparse numeric matrices (i.e., `dgcMatrix` objects from the [Matrix](https://cran.r-project.org/package=Matrix) package)
    
    - Estimated time to completion!!

]

.pull-right[

* [party/partykit](https://cran.r-project.org/web/packages/party/index.html)

    - Uses *conditional inference trees* as the base learners
    
    - Unbiased variable selection
    
    - Can be slow on large data sets

* [bigrf](https://github.com/aloysius-lim/bigrf)

  - Classification only

  - Currently **ORPHANED** on CRAN
  
  - Disk caching using [`bigmmory`](https://cran.r-project.org/web/packages/bigmemory/)

* [Rborist](https://github.com/suiji/Arborist)

    - Offers parallel, distributed tree construction

* [h2o](https://github.com/h2oai/h2o-3/tree/master/h2o-r)

  - Distributed random forests via cloud computing
  
  - Can be stacked with other [`h2o`](https://github.com/h2oai/h2o-3/tree/master/h2o-r) models, like GBMs and DNNs!!

]

]


---

# Predicting email spam

.pull-left[

```{r}
# Load required packages
library(randomForest)

# Fit a random forest
xtst <- subset(tst, select = -type)
set.seed(1633)  # reproducibility
spam_rf <- randomForest(
  type ~ ., 
  data = trn, 
  ntree = 250,
  mtry = 7,  # floor(sqrt(p))  #<<
  xtest = xtst,
  ytest = tst$type,
  keep.forest = TRUE
)
```

]

.pull-right[

```{r, echo=FALSE, fig.width=4, fig.asp=1, out.width="100%"}
# Compute test error
pred <- predict(spam_rf, newdata = xtst, type = "class")
spam_rf_acc <- accuracy(pred = pred, obs = tst$type)

# Tidy test error data
df1 <- spam_bag$test$err.rate %>%
  as.data.frame() %>%
  dplyr::select(error = Test) %>%
  tibble::rowid_to_column("num_trees") %>%
  dplyr::mutate(
    type = "Bagging"
  )
df2 <- spam_rf$test$err.rate %>%
  as.data.frame() %>%
  dplyr::select(error = Test) %>%
  tibble::rowid_to_column("num_trees") %>%
  dplyr::mutate(
    type = "Random forest"
  )
df <- dplyr::bind_rows(df1, df2)

# Animate test error progression
p <- ggplot(df, aes(x = num_trees, y = error, color = type)) +
  geom_path() +
  scale_colour_brewer(palette = "Dark2") +
  geom_hline(yintercept = 1 - spam_tree_acc, linetype = 2, color = "black") +
  geom_hline(yintercept = 1 - spam_bag_acc, linetype = 2, color = dark2[1L]) +
  geom_hline(yintercept = 1 - spam_rf_acc, linetype = 2, color = dark2[2L]) +
  theme_light() +
  theme(legend.title = element_blank(),
        legend.position = c(0.9, 0.9)) +
  xlab("Number of trees") +
  ylab("Test error") +
  ylim(0.04, 0.11) +
  transition_reveal(id = type, along = num_trees)
animate(p, renderer = gifski_renderer(), device = "png")
```

]


---
class: clear

# Random forest

.pull-left[

## <span style="color:MediumSeaGreen">Advantages:</span>

* Competitive accuracy

* Supervised **AND** unsupervised learning

* *Out-of-bag* data `r emo::ji("sunglasses")`

    - Free cross-validation

    - Novel variable importance

* Novel <span style="color:Purple">proximity matrix</span> `r emo::ji("sunglasses")`

    - Outlier/novelty detection
    
    - Multi-dimensional scaling
    
    - Missing value imputation

]

.pull-right[

## <span style="color:Tomato">Disadvantages:</span>

* Missing values (**why?**)

* Can be slow on large data sets since trees are intentionally overgrown (i.e., deep)

    - $m_{try}$ mitigates this issue to some extent

]


---

# Out-of-bag data

For large enough $N$, on average, $1 - e^{-1} \approx 63.21$% or the original records end up in any bootstrap sample

.code75[

```{r bootstrap}
N <- 100000
set.seed(1537)  # for reproducibility
x <- rnorm(N)
mean(x %in% sample(x, replace = TRUE))  # non-OOB proportion
```

]

* Roughly $e^{-1} \approx 36.79$% of the observations are not used in the construction of a particular tree

* These observations are considered *out-of-bag* (OOB) and can be used to

    - Provide an unbiased assessment of model performance (**sort of an unstructured, but free, cross-validation**)
 
    - Construct novel variable importance measure based on the **predictive strength** of each feature!!


---
class: clear, middle, center
# Predicting email spam

.font150.bold.left[OOB vs. test error: e-mail spam example]
```{r, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
# Compute test error
pred <- predict(spam_rf, newdata = xtst, type = "class")
spam_rf_acc <- accuracy(pred = pred, obs = tst$type)

# Plot test error
par(mar = c(4, 4, 0.1, 0.1))
plot(seq_len(spam_rf$ntree), spam_rf$test$err.rate[, "Test"], type = "l", 
     col = dark2[4L], ylim = c(0.04, 0.11),
     ylab = "Error estimate", xlab = "Number of trees")
lines(seq_len(spam_rf$ntree), spam_rf$err.rate[, "OOB"], type = "l", 
     col = dark2[1L])
abline(h = spam_rf$err.rate[spam_rf$ntree, "OOB"], lty = 2, col = dark2[1L])
abline(h = 1 - spam_rf_acc, lty = 2, col = dark2[4L])
legend("topright", c("Random forest (OOB)", "Random forest (test)"),
       col = c(dark2[c(1, 4)]), lty = c(1, 1))
```


---

# OOB-based variable importance

* **Traditional approach:** Average variable importance scores (i.e., total goodness of split) over all the trees in the ensemble (this is what is done for GBMs)

* **Novel approach:** To estimate the importance of the $k$-th variable, $x_k$:

  1. Record the OOB performance of the model
  
  2. Randomly permute all the values of $x_k$ in the OOB data
  
  3. Recompute the OOB performance of the model
  
  4. The difference between the two OOB performance measures the strength of the structural importance of $x_k$
  
* Fundamentally different as these importance scores are **NOT** based on data seen by the individual trees!


---

# Boston housing example

* Housing data from $N = 506$ census tracts in the city of Boston for the year 1970

* The data violate many classical assumptions like linearity, normality, and constant variance

--

* Harrison and Rubinfeld's housing value equation ( $R^2 = 0.81$ ):

```{r, echo=FALSE, out.width="80%"}
knitr::include_graphics("figures/boston-eqn.png")
```

--

* Nowadays, many supervised learning algorithms can fit the data automatically in seconds (**typically with higher accuracy!**)

* The downfall, however, is some loss of interpretation since these algorithms typically do not produce simple prediction formulas (**but there's still hope!** `r emo::ji("pray")`)


---
class: clear

```{r boston-ranger}
# Load required packages
library(ranger)  # a much faster implementation of random forest  #<<

# Load the (corrected) Boston housing data
data(boston, package = "pdp")
```

.pull-left[

```{r boston-rf-timing}
# Using the randomForest package
set.seed(2007)  # reproducibility
system.time(
  boston_rf <- randomForest(
    cmedv ~ ., data = boston, 
    ntree = 5000,  #<<
    mtry = 5,  #<<
    importance = FALSE  #<<
  )
)
```
```{r boston-rf-rsquared, echo=FALSE}
boston_rf$rsq[boston_rf$ntree]
```

]

.pull-right[

```{r boston-ranger-timing}
# Using the ranger package
set.seed(1652)  # reproducibility
system.time(
  boston_ranger <- ranger(
    cmedv ~ ., data = boston, 
    num.trees = 5000,  #<<
    mtry = 5,  # :/  #<<
    importance = "impurity"  #<<
  )
)
```
```{r boston-ranger-rsquared, echo=FALSE}
boston_ranger$r.squared
```

]

```{r boston-rf, echo=FALSE}
# Refit models with less trees
set.seed(1453)  # for reproducibility
boston_rf <- randomForest(cmedv ~ ., data = boston, ntree = 500,
                          importance = TRUE, proximity = TRUE)
boston_ranger <- ranger(cmedv ~ ., data = boston, num.trees = 500,
                        importance = "impurity")
```


---

# Variable importance plots

.pull-left[

* Most RF packages provide variable importance measures, but not all of them provide variable importance plots (VIPs)

* Enter...[vip](https://koalaverse.github.io/vip/index.html) 

    - Provides a consistent framework for extracting and plotting variable importance scores from many types of ML models

    - `vi()` always returns a [*tibble*](https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html)

    - `vip()` uses [ggplot2](http://ggplot2.org/)

]

.pull-right[

```{r vip-install, eval=FALSE}
# Install from CRAN
install.packages("vip")
```

```{r vip-badge, echo=FALSE, out.width="80%"}
knitr::include_graphics("figures/vip-logo.svg")
```
]

---
class: clear, middle

```{r boston-rf-varImpPlot, fig.width=6, fig.height=4, out.width="80%"}
# Construct variable importance plots (the old way)
par(mfrow = c(1, 2))  # side-by-side plots
varImpPlot(boston_rf, main = "")  # randomForest::varImpPlot()  #<<
```


---
class: clear

```{r boston-rf-vip, fig.width=9, fig.height=4, out.width="90%"}
# Load required packages
library(vip)  # for better (and consistent) variable importance plots

# Construct variable importance plots
p1 <- vip(boston_rf, type = 1) + ggtitle("randomForest")
p2 <- vip(boston_rf, type = 2) + ggtitle("randomForest")
p3 <- vip(boston_ranger) + ggtitle("ranger")
grid.arrange(p1, p2, p3, ncol = 3)  # side-by-side plots #<<
```


---

# Partial dependence plots

.scrollable[

* [Partial dependence plots (PDPs)](https://journal.r-project.org/archive/2017/RJ-2017-016/index.html) help visualize the relationship between a subset of the features (typically 1-3) and the response

* Let $x_1$ be the predictor variable of interest with unique values $\left\{x_{11}, x_{12}, \dots, x_{1k}\right\}$

* The partial dependence of the response on $x_1$ can be constructed as follows:

    1. For $i \in \left\{1, 2, \dots, k\right\}$:
    
        a. Copy the training data and replace the original values of $x_1$ with the constant $x_{1i}$
    
        b. Compute the vector of predicted values from the modified copy of the training data
        
        c. Compute the average prediction to obtain $\bar{f}_1\left(x_{1i}\right)$

    2. Plot the pairs $\left\{x_{1i}, \bar{f}_1\left(x_{1i}\right)\right\}$ for $i = 1, 2, \dotsc, k$

]


---

# Partial dependence plots

.pull-left[

* Not all RF implementations provide support for constructing partial dependence plots (PDPs)

* Enter...[pdp](https://journal.r-project.org/archive/2017/RJ-2017-016/index.html)

    - A consistent way of constructing PDPs (and more) from many types of ML models (not just RFs)

    - Allows for multivariate displays (i.e., interactions) and so much more!!
    
    - Includes options for **parallel processing** and **progress bars** `r emo::ji("sunglasses")`

]

.pull-right[

```{r, eval=FALSE}
# Install from CRAN
install.packages("pdp")
```

```{r, echo=FALSE, out.width="80%"}
knitr::include_graphics("figures/pdp-logo.png")
```

]


---
class: clear, middle

# Partial dependence plots

```{r boston-rf-pdps, fig.width=12, fig.height=4, out.width="100%"}
# Load required packages
library(pdp)

# PDPs for the top two predictors
p1 <- partial(boston_ranger, pred.var = "lstat", plot = TRUE)
p2 <- partial(boston_ranger, pred.var = "rm", plot = TRUE)
p3 <- partial(boston_ranger, pred.var = c("lstat", "rm"),  #<<
              chull = TRUE, plot = TRUE)                   #<<
grid.arrange(p1, p2, p3, ncol = 3)
```


---
class: clear, middle

# Partial dependence plots

.code115[

```{r 3d-plots, echo=TRUE}
# 3-D plots  #<<
pd <- attr(p3, "partial.data")  # no need to recalculate #<<
p1 <- plotPartial(pd, 
  levelplot = FALSE, drape = TRUE, colorkey = FALSE,
  screen = list(z = -20, x = -60)
)
```

```{r ggplot2-plots, echo=TRUE}
# Using ggplot2  #<<
library(ggplot2)
p2 <- autoplot(pd, palette = "magma")
```

```{r ice-curves, echo=TRUE}
# ICE and c-ICE curves  #<<
p3 <- boston_ranger %>%  # %>% is automatically imported!
  partial(pred.var = "rm", ice = TRUE, center = TRUE) %>%
  autoplot(alpha = 0.1)
```

]


---
class: clear, middle

# Partial dependence plots

.code150[

```{r, echo=TRUE, fig.width=12, fig.height=4, out.width="100%"}
# Display all three plots side by side
grid.arrange(p1, p2, p3, ncol = 3)
```

]


---

# Proximity matrix

.font115[

* In random forests (and bagging), **trees are intentionally grown deep**!

]


---
class: clear, middle, center

```{r mushroom-tree-diagram-3, echo=FALSE, out.width="70%"}
# Tree diagram (shallow tree)
prp(cart2,
    type = 4,
    clip.right.labs = FALSE, 
    branch.lwd = 2,
    extra = 1, 
    under = TRUE,
    under.cex = 1.5,
    split.cex = 1.5,
    box.col = c("palegreen3", "pink")[cart2$frame$yval])
```


---

# Proximity matrix

.font115[

* In random forests (and bagging), **trees are intentionally grown deep**!

    1. Initialize the proximity between cases $i$ and $j$ to zero: $prox_{ij} = 0$

    2. After a tree is grown, put all of the data, both training and OOB, down the tree

    3. If cases $i$ and $j$ land in the same terminal node, increase $prox_{ij}$ by one

    4. At the end, normalize $prox_{ij}$ by dividing by the number of trees in the forest

* The pairwise proximities can be used to construct a useful distance matrix

* **Requires the storage of an $N \times N$ matrix!!** `r emo::ji("scream")`

* Only available in [randomForest](https://cran.r-project.org/package=randomForest), [party](https://cran.r-project.org/web/packages/party/index.html), and [bigrf](https://github.com/aloysius-lim/bigrf)

]


---

# Swiss banknote data

```{r banknote-rf}
# Load the data
data(banknote, package = "alr3")

# Fit a random forest
set.seed(1701)  # for reproducibility
banknote_rf <- randomForest(
  as.factor(Y) ~ ., 
  data = banknote, 
  proximity = TRUE  #<<
)

# Print the OOB confusion matrix
banknote_rf$confusion
```


---

# Proximity matrix

```{r heatmap, out.width="50%"}
# Heatmap of proximity-based distance matrix
heatmap(1 - banknote_rf$proximity, col = viridis::plasma(256))
```


---

# Outlier scores

Outlyingness of case $n$ in class $j$ to all other cases in class $j$: $out\left(n\right) = N / \sum_{cl\left(k\right) = j} prox_{ik} ^ 2$

.pull-left[

```{r outlier, eval=FALSE}
# Dot chart of proximity-based outlier scores
outlyingness <- tibble::tibble(
  "out" = outlier(banknote_rf),  #<<
  "obs" = seq_along(out),
  "class" = as.factor(banknote$Y)
)
ggplot(outlyingness, aes(x = obs, y = out)) +
  geom_point(aes(color = class, size = out), alpha = 0.5) +
  geom_hline(yintercept = 10, linetype = 2) +  #<<
  labs(x = "Observation", y = "Outlyingness") +
  theme_light() +
  theme(legend.position = "none")
```

]

.pull-right[

```{r outlier-plot, echo=FALSE, out.width="90%"}
# Dot chart of proximity-based outlier scores
outlyingness <- tibble::tibble(
  "out" = outlier(banknote_rf),  #<<
  "obs" = seq_along(out),
  "class" = as.factor(banknote$Y)
)
ggplot(outlyingness, aes(x = obs, y = out)) +
  geom_point(aes(color = class, size = out), alpha = 0.5) +
  geom_hline(yintercept = 10, linetype = 2) +  #<<
  labs(x = "Observation", y = "Outlyingness") +
  theme_light() +
  theme(legend.position = "none")
```

]


---

# Multi-dimensional scaling

```{r MDS, out.width="45%"}
# Multi-dimensional scaling plot of proximity matrix
MDSplot(banknote_rf, fac = as.factor(banknote$Y), k = 2, cex = 1.5)
```


---

# Missing value imputation

* Random forest offers a novel approach to imputing missing values (i.e., `NA`s)

    1. Start with cheap imputation (e.g., median for continuous features)
    
    2. Run a random forest and obtain the proximity matrix
    
    3. Imputed values are updated using:
    
        - The weighted average of the non-missing observations, where the weights are the proximity (**continuous**)
        
        - The category with the largest average proximity (**categorical**)

    4. Iterate this procedure until convergence (~ 4-6 times seems sufficient)

* Highly computational (requires many random forest runs!!)

<!-- * Resulting **OOB error estimate tends to be overly optimistic**  -->

* Only available in [randomForest](https://cran.r-project.org/package=randomForest)


---
class: clear, middle, center

background-image: url(https://upload.wikimedia.org/wikipedia/commons/9/99/Fog_forrest_frickberg.jpg)

.font300.white[Boosting]

???

Image credit: [Wikimedia Commons](https://commons.wikimedia.org/wiki/Forest#/media/File:Fog_forrest_frickberg.jpg)


---

# Background

.font125[

* Boosting is a general technique for creating an ensemble of models (*meta-algorithm*)

* In contrast to bagging, boosting:

    - Uses a *weak learner* for the base learners
    
    - Builds the models sequentially (fix past mistakes)
    
* Most commonly used with **shallow decision trees** (e.g., decision stumps), but other types of models can be boosted

* **Boosting requires more tuning**, but is typically faster and more accurate than RFs!

]


---

# Background

.font125[

* **Many flavors of boosting:** AdaBoost, Real AdaBoost, gradient boosting, and stochastic gradient boosting (GBM), for example

* GBMs are the most **general** and widely used flavor of boosting

    - Introduces subsampling into the rows and columns (**stochastic**)

    - Loss function is optimized using *gradient descent* (**gradient**)

    - With the right combination of parameters, GBMs can emulate RFs!

]


---

# GBMs with squared-error loss

1. Given a training sample $\boldsymbol{d} = \left(\boldsymbol{X}, \boldsymbol{y}\right)$. Fix the number of steps $B$, the shrinkage factor $\epsilon$, and the tree depth $d$. Set the initial fit $\widehat{G}_0 \equiv 0$, and the residual vector $\boldsymbol{r} = \boldsymbol{y}$.

2. For $b = 1, 2, \dots, B$ repeat:

    a) Fit a regression tree $\tilde{g}_b$ to the data $\left(\boldsymbol{X}, \boldsymbol{r}\right)$, grown best-first to depth $d$
    
    b) Update the fitted model with a shrunken version of $\tilde{g}_b$: $\widehat{G}_b = \widehat{G}_{b - 1} + \epsilon \tilde{g}_b$
    
    c) Update the residuals accordingly: $r_i = r_i - \epsilon \tilde{g}_b$, $i = 1, 2, \dots, n$
    
3. Return the sequence of fitted functions $\widehat{G}_b$, $b = 1, 2, \dots, B$.

* Here, $B$, $\epsilon$, and $d$ are tuning parameters

.font150.center[

[Poor mans GBM code](https://github.com/bgreenwell/intro-ml-r/blob/master/code/rpartBoost.R)

]

???

Source: [Computer Age Statistical Inference, pg. 333](https://web.stanford.edu/~hastie/CASI/)


---
class: clear, center, middle

.center.font200[Boosting regression stumps]

![](gifs/boosted_stumps.gif)


---

# Some GBM packages in R

.scrollable[

* [gbm](https://github.com/gbm-developers/gbm): **g**eneralized **b**oosted **m**odels

    - The original R implementation of GMBs (by Greg Ridgeway)
    
    - Slower than modem implementations (but still pretty fast)
    
    - Provides OOB error estimate
    
    - Supports the *weighted tree traversal* method for fast construction of PDPs
    
    - Currently orphaned on CRAN (for now)
    
* [gbm3](https://github.com/gbm-developers/gbm3): **g**eneralized **b**oosted **m**odels

    - Shiny new version of `gbm` that is not backwards compatible
    
    - Faster and supports parallel tree building
    
    - Not currently listed on CRAN
    
* [xgboost](https://github.com/dmlc/xgboost): <span style="color:DeepSkyBlue">eXtreme Gradient Boosting</span>

    - Widely used by data scientists to achieve **state-of-the-art** results across many machine learning challenges (e.g., Kaggle)

    - Designed to be highly efficient, flexible, and portable
    
    - Supports user-defined loss functions
    
    - Supports early stopping
    
    - Includes fast histogram method for numeric features
    
    - Parallel tree boosting and GPU support!!

```{r gpu, echo=FALSE, out.width="50%"}
knitr::include_graphics("figures/gpu.jpg")
```


* [lightgbm](https://github.com/Microsoft/LightGBM):  

    - Microsoft supported, and somewhat [faster](https://github.com/Microsoft/LightGBM/blob/master/docs/Experiments.rst#comparison-experiment), alternative to **xgboost**

    - Uses *leaf-wise* (i.e., best-first) tree growth
    
    - Not currently listed on CRAN

* [mboost](https://github.com/boost-R): **m**odel-based **boost**ing

    - Implements tools for fitting and evaluating a variety of regression and classification models (somewhere between GzLMs/GAMs and GBMs)

]


---

# Missing values

.font140[

* Unlike RFs, GBMs typically support missing values!

* In most GBM implementations, `NA`s are interpreted as containing information (i.e., missing for a reason), rather than missing at random

    - In [gbm](https://github.com/gbm-developers/gbm) and [gbm3](https://github.com/gbm-developers/gbm3), a separate branch is created at each split for `NA`s (i.e., each decision node contains three splits: left, right, and missing)
    
    - In [h2o](https://github.com/h2oai/h2o-3/tree/master/h2o-r) and [xgboost](https://github.com/dmlc/xgboost) each split has a **default direction** that is learned during training (`NA`s take the default route!)

]


---
class: clear, middle

# Boston housing example

.code110[

```{r boston_gbm}
# Load required packages
library(gbm)

# Fit a GBM to the Boston housing data
set.seed(1053)  # for reproducibility
boston_gbm <- gbm(
  cmedv ~ ., 
  data = boston, 
  var.monotone = NULL,        #<<
  distribution = "gaussian",  # "benoulli", "coxph", etc. #<<
  n.trees = 10000,            #<<
  interaction.depth = 5,      #<<
  n.minobsinnode = 10,        #<<
  shrinkage = 0.005,          #<<
  bag.fraction = 1,           #<<
  train.fraction = 1,         #<<
  cv.folds = 10  # k-fold CV often gives the best results
)
```

]


---
class: clear, middle

# Boston housing example

```{r boston_gbm_best, fig.width=6, fig.asp=0.618, out.width="90%"}
best_iter <- gbm.perf(  # how many trees?
  boston_gbm, 
  method = "cv"  # or "OOB" or "test" #<<
)
```


---

# Partial dependence plots

.pull-left[

## Brute force method:
```{r randomForest-pdp-timing}
system.time(
  pd1 <- partial(
    boston_gbm, 
    pred.var = c("lon", "nox"),
    recursive = FALSE,  #<<
    chull = TRUE, 
    n.trees = best_iter  #<<
  )
)
```

]

.pull-right[

## Recursive method:
```{r gbm-pdp-timing}
system.time(
  pd2 <- partial(
    boston_gbm, 
    pred.var = c("lon", "nox"),
    recursive = TRUE,  #<<
    chull = TRUE, 
    n.trees = best_iter  #<<
  )
)
```

]


---

# Partial dependence plots

```{r boston-pdps, fig.width=8, fig.height=4}
# Display plots side by side
grid.arrange(autoplot(pd1), autoplot(pd2), ncol = 2)
```


---

# Ames housing data

.pull-left.font120[

* A contemporary alternative to the often cited Boston housing data

* Contains the sale of individual residential property in Ames, Iowa from 2006 to 2010

* Contains $N = 2930$ observations and $p = 80$ features involved in assessing home values

* Lots of potential for *feature engineering*!!

]

.pull-right[

```{r ames-densities, fig.width=6, fig.asp=0.618, out.width="100%"}
ggplot(AmesHousing::make_ames(), aes(x = Sale_Price, y = Overall_Qual)) + 
  ggridges::geom_density_ridges(aes(fill = Overall_Qual)) +  #<<
  scale_x_continuous(labels = scales::dollar) +
  labs(x = "Sale price", y = "Overall quality") +
  theme_light() + theme(legend.position = "none")
```
.font80[**Source:** [Bradley Boehmke](https://github.com/bradleyboehmke)]

]


---
class: clear, middle

# Ames housing data

.scrollable[

```{r ames-xgboost}
# Load required packages
library(xgboost)

# Construct data set
ames <- AmesHousing::make_ames()

# Feature matrix  # or xgb.DMatrix or sparse matrix  #<<
X <- data.matrix(subset(ames, select = -Sale_Price))

# Fit an XGBoost model
set.seed(203)  # for reproducibility
ames_xgb <- xgboost(         # tune using `xgb.cv()`
  data = X, 
  label = ames$Sale_Price, 
  objective = "reg:linear",  # loss function #<<
  nrounds = 2771,            # number of trees  #<<
  max_depth = 5,             # interaction depth  #<<
  eta = 0.01,                # learning rate  #<<
  subsample = 1,             #<<
  colsample = 1,             #<<
  num_parallel_tree = 1,     #<<
  eval_metric = "rmse",      #<<
  verbose = 0,
  save_period = NULL         #<<
)
```

]


---
class: clear, middle

```{r ames-xgboost-cv, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
# Use k-fold cross-validation to find the "optimal" number of trees
set.seed(1214)  # for reproducibility
ames_xgb_cv <- xgb.cv(
  data = X, 
  label = ames$Sale_Price, 
  objective = "reg:linear",
  nrounds = 10000, 
  max_depth = 5, 
  eta = 0.01, 
  subsample = 1,          
  colsample = 1,          
  num_parallel_tree = 1,  
  eval_metric = "rmse",   
  early_stopping_rounds = 50,
  verbose = 0,
  nfold = 5
)

# Plot cross-validation results
plot(test_rmse_mean ~ iter, data = ames_xgb_cv$evaluation_log, type = "l", 
     ylim = c(0, 200000), xlab = "Number of trees", ylab = "RMSE",
     main = "Results from using xgb.cv()")
lines(train_rmse_mean ~ iter, data = ames_xgb_cv$evaluation_log, col = "red2")
abline(v = ames_xgb_cv$best_iteration, lty = 2)
legend("topright", legend = c("Train", "CV"), lty = 1, col = c("red2", 1),
       inset = 0.15)
```

---

# Ames housing data

```{r xgboost-vip, fig.width=12, fig.height=4}
# Variable importance plots
p1 <- vip(ames_xgb, feature_names = colnames(X), type = "Gain")
p2 <- vip(ames_xgb, feature_names = colnames(X), type = "Cover")
p3 <- vip(ames_xgb, feature_names = colnames(X), type = "Frequency")
grid.arrange(p1, p2, p3, ncol = 3)
```


---

# Ames housing data

.pull-left[

```{r xgboost-vip-2, eval=FALSE}
# By default, `vip()` plots the 
# top 10 features
vip(
  object = ames_xgb, 
  feature_names = colnames(X), 
  type = "Gain",
  num_features = 20, #nrow(X), 
  bar = FALSE
)
```

]

.pull-right[

```{r xgboost-vip-3, echo=FALSE, fig.height=5, fig.width=4, out.width="100%"}
# By default, `vip()` plots the 
# top 10 features
vip(
  object = ames_xgb, 
  feature_names = colnames(X), 
  type = "Gain",
  num_features = 20, #nrow(X), 
  bar = FALSE
)
```

]


---

# Ames housing data

```{r xgboost-pdp, fig.width=12, fig.height=4, out.width="90%"}
# Partial dependence plots
oq_ice <- partial(ames_xgb, pred.var = "Overall_Qual", ice = TRUE, 
                  center = TRUE, train = X)
p4 <- autoplot(partial(ames_xgb, pred.var = "Gr_Liv_Area", train = X))
p5 <- autoplot(partial(ames_xgb, pred.var = "Garage_Cars", train = X))
p6 <- autoplot(oq_ice, alpha = 0.1)
grid.arrange(p4, p5, p6, ncol = 3)
```


---
class: clear, middle, center

# Ames housing data

.font200.center[Variable effect and variable importance]

```{r xgboost-pdp-vi, echo=FALSE, fig.width=9, fig.height=6, out.width="100%"}
# Partial dependence plots for the top/bottom three features
ames_vi <- vi(ames_xgb, feature_names = colnames(X))
feats <- c(ames_vi$Variable[1:3], ames_vi$Variable[21:23])
pds <- lapply(feats, FUN = function(x) {
  pd <- cbind(x, partial(ames_xgb, pred.var = x, train = X))
  names(pd) <- c("xvar", "xval", "yhat")
  pd
})
pds <- do.call(rbind, pds)
ggplot(pds, aes(x = xval, y = yhat)) +
  geom_line(size = 1.5) +
  geom_hline(yintercept = mean(ames$Sale_Price), linetype = 2, col = "red2") +
  facet_wrap( ~ xvar, scales = "free_x") +
  labs(x = "", y = "Partial dependence") +
  theme_light()
```


---

# Summary

.scrollable[

* Check out the [machine learning task view](https://cran.r-project.org/web/views/MachineLearning.html)!

.pull-left[

## <span style="color:MediumSeaGreen">Random forests:</span>

* Builds an ensemble of fully grown decision trees (**low bias, high variance**)

    - Correlation between trees is reduced through subsampling the columns

    - Variance is reduced through averaging
    
* Nearly automatic

* Lots of cool bells and whistles

* Trees are independently grown (embarrassingly parallel)

]

.pull-right[

## <span style="color:MediumSeaGreen">Gradient boosting machines:</span>

* Builds an ensemble of small decision trees (**high bias, low variance**)

    - Bias is reduced through sequential learning and fixing past mistakes

* Requires more tuning, but can be more accurate

* Flexible loss functions

* Trees are grown **NOT** independent, but training times are usually pretty fast since trees are not grown too deep

]

]


---
class: clear, middle

.pull-left.font100.left[

```{r leo-breiman, echo=FALSE, out.width="60%"}
knitr::include_graphics("figures/leo-breiman.jpg")
```

"Take the output of random forests not as absolute truth, but as smart computer generated guesses that may be helpful in leading to a deeper understanding of the problem."

--- Leo Breiman

]

.pull-right.font100.left[

```{r george-box, echo=FALSE, out.width="54%"}
knitr::include_graphics("figures/george-box.jpg")
```

"All models are wrong, but some are useful."

--- George E. Box

]


---
class: middle

# Questions?

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("figures/questions.png")
```
